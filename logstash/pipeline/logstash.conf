# ===============================
# Logstash pipeline configuration
# ===============================

# -------------------------------
# 1. INPUT
# -------------------------------
# Read from the host-mounted logs directory (./logs -> /logs in the container)
input {
  file {
    path => "/logs/access.log"
    start_position => "beginning"        # read the file from the top on first run
    sincedb_path => "/dev/null"          # disable file state persistence (for demo/dev)
    type => "nginx_access"               # tag for later filtering
  }

  file {
    path => "/logs/error.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "nginx_error"
  }
}

# -------------------------------
# 2. FILTERS
# -------------------------------
filter {
  # ----- Parse access.log -----
  if [type] == "nginx_access" {
    grok {
      match => {
        "message" => [
          # Standard Nginx access log format:
          # 127.0.0.1 - - [26/Aug/2025:19:00:00 +0000] "GET /hello HTTP/1.1" 200 123 "-" "curl/8.1.0"
          "%{IPORHOST:client_ip} - %{DATA:ident} %{DATA:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:response:int} (?:%{NUMBER:bytes:int}|-) \"%{DATA:referrer}\" \"%{DATA:agent}\""
        ]
      }
    }

    # Convert the timestamp to @timestamp
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
      remove_field => [ "timestamp" ]
    }
  }

  # ----- Parse error.log -----
  if [type] == "nginx_error" {
    grok {
      match => {
        "message" => [
          # Example error log:
          # 2025/08/26 19:00:00 [error] 1234#5678: *1 open() "/var/www/html/health" failed (2: No such file or directory), client: 127.0.0.1
          "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\] %{NUMBER:pid:int}#%{NUMBER:tid:int}: \*%{NUMBER:connection_id:int} %{GREEDYDATA:error_message}"
        ]
      }
    }

    date {
      match => [ "timestamp", "YYYY/MM/dd HH:mm:ss" ]
      target => "@timestamp"
      remove_field => [ "timestamp" ]
    }
  }
}

# -------------------------------
# 3. OUTPUT
# -------------------------------
output {
  # Print to stdout (useful for debugging)
  stdout {
    codec => rubydebug
  }

  # Send to Elasticsearch
  elasticsearch {
    hosts => [ "http://elasticsearch:9200" ]  # internal service name in docker-compose
    user  => "elastic"
    password => "${ELASTIC_PASSWORD}"         # read from environment
    ilm_enabled => false                      # disable index lifecycle for simplicity
    index => "%{[type]}-%{+YYYY.MM.dd}"       # daily index per log type
  }
}
